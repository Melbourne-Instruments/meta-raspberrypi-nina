diff --git a/drivers/dma/bcm2835-dma.c b/drivers/dma/bcm2835-dma.c
index 5a9b18edc2b7..5342e6716e46 100644
--- a/drivers/dma/bcm2835-dma.c
+++ b/drivers/dma/bcm2835-dma.c
@@ -32,6 +32,7 @@
 #include <linux/spinlock.h>
 #include <linux/of.h>
 #include <linux/of_dma.h>
+#include <rtdm/driver.h>
 
 #include "virt-dma.h"
 
@@ -311,6 +312,12 @@ static const struct bcm2835_dma_cfg_data bcm2711_dma_cfg = {
 	.dma_mask = DMA_BIT_MASK(36),
 };
 
+/* RTDM interrupt objects */
+static rtdm_irq_t  dma_rtdm_tx1_irq;
+static rtdm_irq_t  dma_rtdm_rx1_irq;
+static rtdm_irq_t  dma_rtdm_tx2_irq;
+static rtdm_irq_t  dma_rtdm_rx2_irq;
+
 static inline size_t bcm2835_dma_max_frame_length(struct bcm2835_chan *c)
 {
 	/* lite and normal channels have different max frame length */
@@ -682,6 +689,56 @@ static void bcm2835_dma_start_desc(struct bcm2835_chan *c)
 	}
 }
 
+static void bcm2835_chan_complete_cyclic(struct virt_dma_desc *vd)
+{
+	struct virt_dma_chan *vc = to_virt_chan(vd->tx.chan);
+
+	vc->cyclic = vd;
+	vchan_complete((unsigned long)vc);
+}
+
+static void bcm2835_chan_complete(struct virt_dma_desc *vd)
+{
+	struct virt_dma_chan *vc = to_virt_chan(vd->tx.chan);
+	dma_cookie_t cookie;
+
+	cookie = vd->tx.cookie;
+	dma_cookie_complete(&vd->tx);
+	dev_vdbg(vc->chan.device->dev, "txd %p[%x]: marked complete\n",
+		 vd, cookie);
+	list_add_tail(&vd->node, &vc->desc_completed);
+
+	vchan_complete((unsigned long)vc);
+}
+
+static int bcm2835_dma_rtcallback(rtdm_irq_t *irqh)
+{
+	struct bcm2835_chan *c = rtdm_irq_get_arg(irqh, struct bcm2835_chan);
+	struct bcm2835_desc *d;
+	unsigned long flags;
+
+	/* Acknowledge interrupt */
+	writel(BCM2835_DMA_INT, c->chan_base + BCM2835_DMA_CS);
+
+	d = c->desc;
+
+	if (d) {
+		if (d->cyclic) {
+			/* call the cyclic callback */
+			bcm2835_chan_complete_cyclic(&d->vd);
+
+			/* Keep the DMA engine running */
+			writel(BCM2835_DMA_ACTIVE,
+			       c->chan_base + BCM2835_DMA_CS);
+		} else {
+			bcm2835_chan_complete(&c->desc->vd);
+			bcm2835_dma_start_desc(c);
+		}
+	}
+
+	return RTDM_IRQ_HANDLED;
+}
+
 static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 {
 	struct bcm2835_chan *c = data;
@@ -697,7 +754,7 @@ static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 			return IRQ_NONE;
 	}
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 
 	/*
 	 * Clear the INT flag to receive further interrupts. Keep the channel
@@ -721,7 +778,7 @@ static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 		}
 	}
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 
 	return IRQ_HANDLED;
 }
@@ -748,15 +805,61 @@ static int bcm2835_dma_alloc_chan_resources(struct dma_chan *chan)
 			   c->irq_flags, "DMA IRQ", c);
 }
 
+/* This handler was not present for bcm2835-dma and most certainly no one calls
+it. Hence, it's safe to use this to enter dma backend to get the rtdm irqs */
+static int bcm2835_dma_resume(struct dma_chan *chan)
+{
+	static rtdm_irq_t *rtdm_irq_handle;
+	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
+	int ret = 0;
+
+	if (chan->private) {
+		if (!strcmp(chan->private, "rtdm-tx1-irq")) {
+			rtdm_irq_handle = &dma_rtdm_tx1_irq;
+		} else if (!strcmp(chan->private, "rtdm-rx1-irq")) {
+			rtdm_irq_handle = &dma_rtdm_rx1_irq;
+		} else if (!strcmp(chan->private, "rtdm-tx2-irq")) {
+			rtdm_irq_handle = &dma_rtdm_tx2_irq;
+		} else if (!strcmp(chan->private, "rtdm-rx2-irq")) {
+			rtdm_irq_handle = &dma_rtdm_rx2_irq;
+		} else {
+			return ret;
+		}
+		free_irq(c->irq_number, c);
+		ret = rtdm_irq_request(rtdm_irq_handle, c->irq_number,
+					bcm2835_dma_rtcallback,
+					0, "RT DMA IRQ", c);
+		if (ret) {
+			printk(KERN_ERR "bcm2835-dma: RT IRQ request failed %d %d\n", ret, c->irq_number);
+		}
+	}
+	return ret;
+}
+
 static void bcm2835_dma_free_chan_resources(struct dma_chan *chan)
 {
 	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
 
 	vchan_free_chan_resources(&c->vc);
+	if (chan->private) {
+		if (!strcmp(chan->private, "rtdm-tx1-irq")) {
+			rtdm_irq_free(&dma_rtdm_tx1_irq);
+			goto exit;
+		} else if (!strcmp(chan->private, "rtdm-rx1-irq")) {
+			rtdm_irq_free(&dma_rtdm_rx1_irq);
+			goto exit;
+		} else if (!strcmp(chan->private, "rtdm-tx2-irq")) {
+			rtdm_irq_free(&dma_rtdm_tx2_irq);
+			goto exit;
+		} else if (!strcmp(chan->private, "rtdm-rx2-irq")) {
+			rtdm_irq_free(&dma_rtdm_rx2_irq);
+			goto exit;
+		}
+	}
 	free_irq(c->irq_number, c);
-	dma_pool_destroy(c->cb_pool);
 
-	dev_dbg(c->vc.chan.device->dev, "Freeing DMA channel %u\n", c->ch);
+exit:	dma_pool_destroy(c->cb_pool);
+	dev_dbg(c->vc.chan.device->dev, "Freeing DMA channel %u\n",c->ch);
 }
 
 static size_t bcm2835_dma_desc_size(struct bcm2835_desc *d)
@@ -800,7 +903,7 @@ static enum dma_status bcm2835_dma_tx_status(struct dma_chan *chan,
 	if (ret == DMA_COMPLETE || !txstate)
 		return ret;
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 	vd = vchan_find_desc(&c->vc, cookie);
 	if (vd) {
 		txstate->residue =
@@ -829,7 +932,7 @@ static enum dma_status bcm2835_dma_tx_status(struct dma_chan *chan,
 		txstate->residue = 0;
 	}
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 
 	return ret;
 }
@@ -839,11 +942,11 @@ static void bcm2835_dma_issue_pending(struct dma_chan *chan)
 	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
 	unsigned long flags;
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 	if (vchan_issue_pending(&c->vc) && !c->desc)
 		bcm2835_dma_start_desc(c);
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 }
 
 static struct dma_async_tx_descriptor *bcm2835_dma_prep_dma_memcpy(
@@ -1047,7 +1150,7 @@ static int bcm2835_dma_terminate_all(struct dma_chan *chan)
 	unsigned long flags;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 
 	/* stop DMA activity */
 	if (c->desc) {
@@ -1060,7 +1163,7 @@ static int bcm2835_dma_terminate_all(struct dma_chan *chan)
 	}
 
 	vchan_get_all_descriptors(&c->vc, &head);
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 	vchan_dma_desc_free_list(&c->vc, &head);
 
 	return 0;
@@ -1243,6 +1346,7 @@ static int bcm2835_dma_probe(struct platform_device *pdev)
 	dma_cap_set(DMA_PRIVATE, od->ddev.cap_mask);
 	dma_cap_set(DMA_CYCLIC, od->ddev.cap_mask);
 	dma_cap_set(DMA_MEMCPY, od->ddev.cap_mask);
+	od->ddev.device_resume = bcm2835_dma_resume;
 	od->ddev.device_alloc_chan_resources = bcm2835_dma_alloc_chan_resources;
 	od->ddev.device_free_chan_resources = bcm2835_dma_free_chan_resources;
 	od->ddev.device_tx_status = bcm2835_dma_tx_status;
diff --git a/drivers/dma/virt-dma.c b/drivers/dma/virt-dma.c
index 256fc662c500..1721ab5ba6ce 100644
--- a/drivers/dma/virt-dma.c
+++ b/drivers/dma/virt-dma.c
@@ -23,11 +23,11 @@ dma_cookie_t vchan_tx_submit(struct dma_async_tx_descriptor *tx)
 	unsigned long flags;
 	dma_cookie_t cookie;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	cookie = dma_cookie_assign(tx);
 
 	list_move_tail(&vd->node, &vc->desc_submitted);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	dev_dbg(vc->chan.device->dev, "vchan %p: txd %p[%x]: submitted\n",
 		vc, vd, cookie);
@@ -52,9 +52,9 @@ int vchan_tx_desc_free(struct dma_async_tx_descriptor *tx)
 	struct virt_dma_desc *vd = to_virt_desc(tx);
 	unsigned long flags;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	list_del(&vd->node);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	dev_dbg(vc->chan.device->dev, "vchan %p: txd %p[%x]: freeing\n",
 		vc, vd, vd->tx.cookie);
@@ -80,7 +80,7 @@ EXPORT_SYMBOL_GPL(vchan_find_desc);
  * This tasklet handles the completion of a DMA descriptor by
  * calling its callback and freeing it.
  */
-static void vchan_complete(unsigned long arg)
+void vchan_complete(unsigned long arg)
 {
 	struct virt_dma_chan *vc = (struct virt_dma_chan *)arg;
 	struct virt_dma_desc *vd, *_vd;
@@ -108,6 +108,7 @@ static void vchan_complete(unsigned long arg)
 		vchan_vdesc_fini(vd);
 	}
 }
+EXPORT_SYMBOL_GPL(vchan_complete);
 
 void vchan_dma_desc_free_list(struct virt_dma_chan *vc, struct list_head *head)
 {
@@ -129,7 +130,7 @@ void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev)
 {
 	dma_cookie_init(&vc->chan);
 
-	spin_lock_init(&vc->lock);
+	raw_spin_lock_init(&vc->lock);
 	INIT_LIST_HEAD(&vc->desc_allocated);
 	INIT_LIST_HEAD(&vc->desc_submitted);
 	INIT_LIST_HEAD(&vc->desc_issued);
diff --git a/drivers/dma/virt-dma.h b/drivers/dma/virt-dma.h
index ab158bac03a7..f169e6f00c29 100644
--- a/drivers/dma/virt-dma.h
+++ b/drivers/dma/virt-dma.h
@@ -24,7 +24,11 @@ struct virt_dma_chan {
 	struct tasklet_struct task;
 	void (*desc_free)(struct virt_dma_desc *);
 
-	spinlock_t lock;
+#ifdef CONFIG_IPIPE
+	ipipe_spinlock_t lock;
+#else
+	raw_spinlock_t lock;
+#endif
 
 	/* protected by vc.lock */
 	struct list_head desc_allocated;
@@ -46,6 +50,7 @@ void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev);
 struct virt_dma_desc *vchan_find_desc(struct virt_dma_chan *, dma_cookie_t);
 extern dma_cookie_t vchan_tx_submit(struct dma_async_tx_descriptor *);
 extern int vchan_tx_desc_free(struct dma_async_tx_descriptor *);
+extern void vchan_complete(unsigned long arg);
 
 /**
  * vchan_tx_prep - prepare a descriptor
@@ -66,9 +71,9 @@ static inline struct dma_async_tx_descriptor *vchan_tx_prep(struct virt_dma_chan
 	vd->tx_result.result = DMA_TRANS_NOERROR;
 	vd->tx_result.residue = 0;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	list_add_tail(&vd->node, &vc->desc_allocated);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	return &vd->tx;
 }
@@ -187,11 +192,11 @@ static inline void vchan_free_chan_resources(struct virt_dma_chan *vc)
 	unsigned long flags;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	vchan_get_all_descriptors(vc, &head);
 	list_for_each_entry(vd, &head, node)
 		dmaengine_desc_clear_reuse(&vd->tx);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	vchan_dma_desc_free_list(vc, &head);
 }
